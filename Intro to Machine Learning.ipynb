{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification in Machine Learning\n",
    "I will investigte several datasets, and fit them to the Naive Bayes model. I will also write about the machile learning algorhythms and other theory behing this amazing theme. I'm really excited!\n",
    "\n",
    "Supervised classification models use datasets which were already labeled, and then using that information to make predictions on new data.\n",
    "\n",
    "Firs of all some example of machine learning include:\n",
    "1. Training a musical app (like Spotify) or on-demand tv app (like Netflix) to give suggestions based on content the the user has previously liked or disliked.\n",
    "2. Training a webapp (like Facebook) to make suggestions to tag people in new uploaded photos, using album of tagged photographs and facial recognition.\n",
    "\n",
    "## Features & Labels\n",
    "* **Features** are input, extracted from the raw training data. There are identified during data exploration. E.g.: Music tempo (bps), intensity OR Terrain type (steep-flat, smooth-very uneven).   \n",
    "* **Labels** are the product classifications from fiting the classification model. E.g.: User: Like or Dislike OR Speed of vehicle.\n",
    "\n",
    "**Scatter plots** are perfect for visualising data to see which classification model is required in training. The results of a fit model will also appear in the scatter plot. From scatter plots to predictions, Next step is to make predictions, i.e. what would a classification be for a completely new poin in the scatter plot?\n",
    "\n",
    "In order to maximise predictions classification models define **decision surface** which is a border separation distinct labels.\n",
    "\n",
    "# Naive Bayes\n",
    "This [clasifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) defines decision boundarry which helps divide the labels.\n",
    "useful explanation [here](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/). I will be using Python scikit learn library [Gaussian Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) classifier.\n",
    "\n",
    "_Note:_ Following are the steps to take the classifier:  \n",
    "1. Divide the featured data into training set (about 90%) and test set (about 10%). It is important to always test the model on a dataset which was **not** used in training to avoid **overfitting data** which would produce biased results.\n",
    "2. Import correct libraries, sklearn is one of the most popular libraries for supervised classifiers.\n",
    "3. Create a classifier.\n",
    "4. Fit training data set, alowing machine to learn. Fitting will include providing the Features list and Labels list.\n",
    "5. Make the machine classify new set of data with out providing it with labels.\n",
    "6. Perform model diagnostics, evaluating accuracy of algorhythm, which is defined as: Number of test points that are classified correctly divided by total number of test points.\n",
    "Or simply using inbuilt scorring function:\n",
    "``` python\n",
    "print clf.score(features_test, labels_test)\n",
    "```\n",
    "\n",
    "Model Strengths:\n",
    "* Easy to impleent\n",
    "* Big feature space for text data\n",
    "* Very efficient  \n",
    "\n",
    "Model Weaknesses:\n",
    "* It can break\n",
    "* It ignores word order and groups of words\n",
    "\n",
    "## Example: Identify the author of emails\n",
    "The data provided is from two people Chris and Sara. The objective is to classify the emails as written by one person or the other based only on the text of the email. We will start with Naive Bayes, and then expand in later projects to other algorithms.\n",
    "\n",
    "We will start by giving you a list of strings. Each string is the text of an email, which has undergone some basic preprocessing; we will then provide the code to split the dataset into training and testing sets. (In the next lessons you’ll learn how to do this preprocessing and splitting yourself, but for now we’ll give the code to you).\n",
    "\n",
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts. In this mini-project, you will download and install sklearn on your computer and use Naive Bayes to classify emails by author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Create a classifier\n",
    "clf = \n",
    "# Fit the data into classifier, in this step the model will learn how to classify the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
